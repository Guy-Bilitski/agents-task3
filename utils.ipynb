{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Quality Metrics Utilities\n",
    "\n",
    "This notebook provides utilities for:\n",
    "1. Calculating spelling error ratios between text pairs\n",
    "2. Computing embedding distances using sentence transformers\n",
    "3. Comprehensive translation quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GuyBilitski\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# Auto-install required packages\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from scipy.spatial.distance import cosine, euclidean\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"scipy\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from scipy.spatial.distance import cosine, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCalculator:\n",
    "    \"\"\"\n",
    "    Singleton-like class for efficient embedding calculations.\n",
    "    Caches the model to avoid reloading on every calculation.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _model = None\n",
    "    _model_name = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def get_model(self, model_name: str = 'all-MiniLM-L6-v2') -> SentenceTransformer:\n",
    "        \"\"\"Load or retrieve cached sentence transformer model.\"\"\"\n",
    "        if self._model is None or self._model_name != model_name:\n",
    "            self._model = SentenceTransformer(model_name)\n",
    "            self._model_name = model_name\n",
    "        return self._model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'[.,!?;:\\'\"()\\[\\]{}<>]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def _calculate_word_difference_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate ratio using symmetric difference of word sets.\n",
    "\n",
    "    This version divides by the size of the union of both word sets so the\n",
    "    returned ratio is always between 0.0 and 1.0 (0% - 100%).\"\"\"\n",
    "    words1 = set(_normalize_text(text1).split())\n",
    "    words2 = set(_normalize_text(text2).split())\n",
    "\n",
    "    if not words1 and not words2:\n",
    "        return 0.0\n",
    "\n",
    "    differing_words = words1.symmetric_difference(words2)\n",
    "    total_unique_words = len(words1.union(words2))\n",
    "\n",
    "    return len(differing_words) / total_unique_words if total_unique_words > 0 else 0.0\n",
    "\n",
    "\n",
    "def _calculate_levenshtein_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate character-level Levenshtein distance ratio.\"\"\"\n",
    "    norm_text1 = _normalize_text(text1)\n",
    "    norm_text2 = _normalize_text(text2)\n",
    "    \n",
    "    ratio = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()\n",
    "    return 1.0 - ratio\n",
    "\n",
    "\n",
    "def _calculate_sequence_matcher_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate error ratio using difflib.SequenceMatcher.\"\"\"\n",
    "    norm_text1 = _normalize_text(text1)\n",
    "    norm_text2 = _normalize_text(text2)\n",
    "    \n",
    "    similarity = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()\n",
    "    return 1.0 - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spelling_error_ratio(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    method: str = 'symmetric_difference'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the ratio of spelling/word errors between two text strings.\n",
    "    \n",
    "    This function compares two texts at the word level to determine how different\n",
    "    they are. Useful for measuring translation quality degradation.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text string (typically the original)\n",
    "        text2: Second text string (typically the final translation)\n",
    "        method: Comparison method to use:\n",
    "            - 'symmetric_difference': Words that appear in one text but not both\n",
    "            - 'levenshtein': Character-level edit distance ratio\n",
    "            - 'sequence_matcher': Uses difflib.SequenceMatcher for similarity\n",
    "    \n",
    "    Returns:\n",
    "        float: Error ratio between 0.0 (identical) and 1.0 (completely different)\n",
    "        \n",
    "    Examples:\n",
    "        >>> calculate_spelling_error_ratio(\"The cat sits\", \"The cat sits\")\n",
    "        0.0\n",
    "        >>> calculate_spelling_error_ratio(\"The cat sits\", \"The dog runs\")\n",
    "        0.6667\n",
    "    \"\"\"\n",
    "    if not text1 and not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    if not text1 or not text2:\n",
    "        return 1.0\n",
    "    \n",
    "    if method == 'symmetric_difference':\n",
    "        return _calculate_word_difference_ratio(text1, text2)\n",
    "    elif method == 'levenshtein':\n",
    "        return _calculate_levenshtein_ratio(text1, text2)\n",
    "    elif method == 'sequence_matcher':\n",
    "        return _calculate_sequence_matcher_ratio(text1, text2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'symmetric_difference', 'levenshtein', or 'sequence_matcher'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_distance(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    return_all_metrics: bool = False\n",
    ") -> float | Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate semantic distance between two texts using sentence embeddings.\n",
    "    \n",
    "    This function uses state-of-the-art sentence transformers to create dense\n",
    "    vector representations of texts and compute their semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text string\n",
    "        text2: Second text string\n",
    "        model_name: Sentence transformer model to use. Options:\n",
    "            - 'all-MiniLM-L6-v2' (default, fast, 384 dimensions)\n",
    "            - 'all-mpnet-base-v2' (slower, more accurate, 768 dimensions)\n",
    "            - 'paraphrase-multilingual-MiniLM-L12-v2' (multilingual)\n",
    "        return_all_metrics: If True, return dict with all distance metrics\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine distance (0.0 = identical, 2.0 = opposite) if return_all_metrics=False\n",
    "        dict: Dictionary with multiple metrics if return_all_metrics=True:\n",
    "            - cosine_distance: 1 - cosine_similarity (0 to 2)\n",
    "            - cosine_similarity: Cosine similarity (-1 to 1)\n",
    "            - euclidean_distance: L2 distance between embeddings\n",
    "            - manhattan_distance: L1 distance between embeddings\n",
    "    \n",
    "    Examples:\n",
    "        >>> calculate_embedding_distance(\"The cat sits\", \"The cat sits\")\n",
    "        0.0\n",
    "        >>> calculate_embedding_distance(\"I love pizza\", \"Pizza is great\")\n",
    "        0.234\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return 1.0 if not return_all_metrics else {\n",
    "            'cosine_distance': 1.0,\n",
    "            'cosine_similarity': 0.0,\n",
    "            'euclidean_distance': 0.0,\n",
    "            'manhattan_distance': 0.0\n",
    "        }\n",
    "    \n",
    "    # Get cached model\n",
    "    calculator = EmbeddingCalculator()\n",
    "    model = calculator.get_model(model_name)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedding1 = model.encode(text1, convert_to_numpy=True)\n",
    "    embedding2 = model.encode(text2, convert_to_numpy=True)\n",
    "    \n",
    "    # Calculate cosine distance and similarity\n",
    "    cos_dist = cosine(embedding1, embedding2)\n",
    "    cos_sim = 1.0 - cos_dist\n",
    "    \n",
    "    if not return_all_metrics:\n",
    "        return float(cos_dist)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    eucl_dist = euclidean(embedding1, embedding2)\n",
    "    manh_dist = np.sum(np.abs(embedding1 - embedding2))\n",
    "    \n",
    "    return {\n",
    "        'cosine_distance': float(cos_dist),\n",
    "        'cosine_similarity': float(cos_sim),\n",
    "        'euclidean_distance': float(eucl_dist),\n",
    "        'manhattan_distance': float(manh_dist)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_translation_quality_metrics(\n",
    "    original: str,\n",
    "    translated: str,\n",
    "    model_name: str = 'all-MiniLM-L6-v2'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive quality metrics for a translation pair.\n",
    "    \n",
    "    This is a convenience function that computes both spelling error ratio\n",
    "    and embedding distance in a single call.\n",
    "    \n",
    "    Args:\n",
    "        original: Original text\n",
    "        translated: Translated text (after full translation chain)\n",
    "        model_name: Sentence transformer model to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all quality metrics:\n",
    "            - spelling_error_ratio: Word-level difference ratio\n",
    "            - embedding_distance: Semantic distance (cosine)\n",
    "            - embedding_similarity: Semantic similarity (1 - distance)\n",
    "    \n",
    "    Examples:\n",
    "        >>> metrics = calculate_translation_quality_metrics(\n",
    "        ...     \"The cat sits on the mat\",\n",
    "        ...     \"A cat is sitting on a mat\"\n",
    "        ... )\n",
    "        >>> print(f\"Error ratio: {metrics['spelling_error_ratio']:.2f}\")\n",
    "        >>> print(f\"Semantic similarity: {metrics['embedding_similarity']:.2f}\")\n",
    "    \"\"\"\n",
    "    error_ratio = calculate_spelling_error_ratio(original, translated)\n",
    "    embedding_dist = calculate_embedding_distance(original, translated, model_name)\n",
    "    \n",
    "    return {\n",
    "        'spelling_error_ratio': float(error_ratio),\n",
    "        'embedding_distance': float(embedding_dist),\n",
    "        'embedding_similarity': float(1.0 - embedding_dist)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### Spelling Error Ratio Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: The cat sits on the mat\n",
      "Text 2: The cat sits on the mat\n",
      "Error Ratio: 0.0000 (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Identical sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"The cat sits on the mat\"\n",
    "ratio = calculate_spelling_error_ratio(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: The cat sits on the mat\n",
      "Text 2: A feline rests on the rug\n",
      "Error Ratio: 0.7778 (77.78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Similar sentences with different words\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"A feline rests on the rug\"\n",
    "ratio = calculate_spelling_error_ratio(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: symmetric_difference      -> Error Ratio: 0.5714\n",
      "Method: levenshtein               -> Error Ratio: 0.1385\n",
      "Method: sequence_matcher          -> Error Ratio: 0.1385\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Different comparison methods\n",
    "text1 = \"The weather is beautiful today\"\n",
    "text2 = \"The weather was beautiful yesterday\"\n",
    "\n",
    "for method in ['symmetric_difference', 'levenshtein', 'sequence_matcher']:\n",
    "    ratio = calculate_spelling_error_ratio(text1, text2, method=method)\n",
    "    print(f\"Method: {method:25s} -> Error Ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Distance Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: The cat sits on the mat\n",
      "Text 2: The cat sits on the mat\n",
      "Embedding Distance: 0.0000\n",
      "Similarity: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Identical sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"The cat sits on the mat\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: The cat sits on the mat\n",
      "Text 2: A feline rests on the rug\n",
      "Embedding Distance: 0.4393\n",
      "Similarity: 56.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Semantically similar sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"A feline rests on the rug\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: I love programming in Python\n",
      "Text 2: The weather is beautiful today\n",
      "Embedding Distance: 0.9339\n",
      "Similarity: 6.61%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Different sentences\n",
    "text1 = \"I love programming in Python\"\n",
    "text2 = \"The weather is beautiful today\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: The quick brown fox jumps over the lazy dog\n",
      "Text 2: The fast brown fox leaps over the lazy dog\n",
      "\n",
      "All Metrics:\n",
      "  cosine_distance          : 0.0257\n",
      "  cosine_similarity        : 0.9743\n",
      "  euclidean_distance       : 0.2269\n",
      "  manhattan_distance       : 3.4938\n"
     ]
    }
   ],
   "source": [
    "# Example 4: All metrics\n",
    "text1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "text2 = \"The fast brown fox leaps over the lazy dog\"\n",
    "metrics = calculate_embedding_distance(text1, text2, return_all_metrics=True)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(\"\\nAll Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key:25s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Quality Metrics Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The cat sits on the mat\n",
      "Back-translated: A cat is sitting on a mat\n",
      "\n",
      "Quality Metrics:\n",
      "  Spelling Error Ratio: 0.6250 (62.50%)\n",
      "  Embedding Distance: 0.0274\n",
      "  Embedding Similarity: 0.9726 (97.26%)\n",
      "\n",
      "Interpretation: A similarity of 97.3% suggests the translation preserved meaning well.\n"
     ]
    }
   ],
   "source": [
    "# Example: Comprehensive translation quality assessment\n",
    "original = \"The cat sits on the mat\"\n",
    "back_translated = \"A cat is sitting on a mat\"\n",
    "\n",
    "metrics = calculate_translation_quality_metrics(original, back_translated)\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"Back-translated: {back_translated}\")\n",
    "print(\"\\nQuality Metrics:\")\n",
    "print(f\"  Spelling Error Ratio: {metrics['spelling_error_ratio']:.4f} ({metrics['spelling_error_ratio']*100:.2f}%)\")\n",
    "print(f\"  Embedding Distance: {metrics['embedding_distance']:.4f}\")\n",
    "print(f\"  Embedding Similarity: {metrics['embedding_similarity']:.4f} ({metrics['embedding_similarity']*100:.2f}%)\")\n",
    "print(f\"\\nInterpretation: A similarity of {metrics['embedding_similarity']*100:.1f}% suggests the translation preserved meaning {'well' if metrics['embedding_similarity'] > 0.7 else 'moderately' if metrics['embedding_similarity'] > 0.5 else 'poorly'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I love programming in Python\n",
      "\n",
      "Translation 1: Python programming is something I enjoy\n",
      "  Similarity: 91.9%, Error Ratio: 62.5%\n",
      "\n",
      "Translation 2: I enjoy coding with Python\n",
      "  Similarity: 92.9%, Error Ratio: 75.0%\n",
      "\n",
      "Translation 3: Programming in Python is fun for me\n",
      "  Similarity: 91.7%, Error Ratio: 66.7%\n",
      "\n",
      "Translation 4: The weather is nice today\n",
      "  Similarity: 8.0%, Error Ratio: 100.0%\n",
      "\n",
      "Translation 4: The weather is nice today\n",
      "  Similarity: 8.0%, Error Ratio: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Compare multiple translations\n",
    "original = \"I love programming in Python\"\n",
    "translations = [\n",
    "    \"Python programming is something I enjoy\",\n",
    "    \"I enjoy coding with Python\",\n",
    "    \"Programming in Python is fun for me\",\n",
    "    \"The weather is nice today\"  # Poor translation\n",
    "]\n",
    "\n",
    "print(f\"Original: {original}\\n\")\n",
    "for i, trans in enumerate(translations, 1):\n",
    "    metrics = calculate_translation_quality_metrics(original, trans)\n",
    "    print(f\"Translation {i}: {trans}\")\n",
    "    print(f\"  Similarity: {metrics['embedding_similarity']*100:.1f}%, Error Ratio: {metrics['spelling_error_ratio']*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"The Trump administration is eager to use the momentum of talks with Ukrainian and Russian officials to try and force both Presidents Volodymyr Zelensky and Vladimir Putin to the table on an initial ceasefire deal, those sources said\"\n",
    "errored_sentence = \"The Trump adminstration is egar to use the momemtum of talks with Ukrianian and Russain officals to try and force both President Volodymyr Zelensky and Vladmir Putin to the tabel on an inital ceasefire deal, thoze sources said.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Ratio: 0.5116 (51.16%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = calculate_spelling_error_ratio(original_sentence, errored_sentence)\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
