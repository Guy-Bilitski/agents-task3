{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Quality Metrics Utilities\n",
    "\n",
    "This notebook provides utilities for:\n",
    "1. Calculating spelling error ratios between text pairs\n",
    "2. Computing embedding distances using sentence transformers\n",
    "3. Comprehensive translation quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# Auto-install required packages\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from scipy.spatial.distance import cosine, euclidean\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"scipy\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from scipy.spatial.distance import cosine, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCalculator:\n",
    "    \"\"\"\n",
    "    Singleton-like class for efficient embedding calculations.\n",
    "    Caches the model to avoid reloading on every calculation.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _model = None\n",
    "    _model_name = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def get_model(self, model_name: str = 'all-MiniLM-L6-v2') -> SentenceTransformer:\n",
    "        \"\"\"Load or retrieve cached sentence transformer model.\"\"\"\n",
    "        if self._model is None or self._model_name != model_name:\n",
    "            self._model = SentenceTransformer(model_name)\n",
    "            self._model_name = model_name\n",
    "        return self._model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'[.,!?;:\\'\"()\\[\\]{}<>]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def _calculate_word_difference_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate ratio using symmetric difference of word sets.\n",
    "\n",
    "    This version divides by the size of the union of both word sets so the\n",
    "    returned ratio is always between 0.0 and 1.0 (0% - 100%).\"\"\"\n",
    "    words1 = set(_normalize_text(text1).split())\n",
    "    words2 = set(_normalize_text(text2).split())\n",
    "\n",
    "    if not words1 and not words2:\n",
    "        return 0.0\n",
    "\n",
    "    differing_words = words1.symmetric_difference(words2)\n",
    "    total_unique_words = len(words1.union(words2))\n",
    "\n",
    "    return len(differing_words) / total_unique_words if total_unique_words > 0 else 0.0\n",
    "\n",
    "\n",
    "def _calculate_levenshtein_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate character-level Levenshtein distance ratio.\"\"\"\n",
    "    norm_text1 = _normalize_text(text1)\n",
    "    norm_text2 = _normalize_text(text2)\n",
    "    \n",
    "    ratio = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()\n",
    "    return 1.0 - ratio\n",
    "\n",
    "\n",
    "def _calculate_sequence_matcher_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate error ratio using difflib.SequenceMatcher.\"\"\"\n",
    "    norm_text1 = _normalize_text(text1)\n",
    "    norm_text2 = _normalize_text(text2)\n",
    "    \n",
    "    similarity = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()\n",
    "    return 1.0 - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spelling_error_ratio(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    method: str = 'symmetric_difference'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the ratio of spelling/word errors between two text strings.\n",
    "    \n",
    "    This function compares two texts at the word level to determine how different\n",
    "    they are. Useful for measuring translation quality degradation.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text string (typically the original)\n",
    "        text2: Second text string (typically the final translation)\n",
    "        method: Comparison method to use:\n",
    "            - 'symmetric_difference': Words that appear in one text but not both\n",
    "            - 'levenshtein': Character-level edit distance ratio\n",
    "            - 'sequence_matcher': Uses difflib.SequenceMatcher for similarity\n",
    "    \n",
    "    Returns:\n",
    "        float: Error ratio between 0.0 (identical) and 1.0 (completely different)\n",
    "        \n",
    "    Examples:\n",
    "        >>> calculate_spelling_error_ratio(\"The cat sits\", \"The cat sits\")\n",
    "        0.0\n",
    "        >>> calculate_spelling_error_ratio(\"The cat sits\", \"The dog runs\")\n",
    "        0.6667\n",
    "    \"\"\"\n",
    "    if not text1 and not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    if not text1 or not text2:\n",
    "        return 1.0\n",
    "    \n",
    "    if method == 'symmetric_difference':\n",
    "        return _calculate_word_difference_ratio(text1, text2)\n",
    "    elif method == 'levenshtein':\n",
    "        return _calculate_levenshtein_ratio(text1, text2)\n",
    "    elif method == 'sequence_matcher':\n",
    "        return _calculate_sequence_matcher_ratio(text1, text2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'symmetric_difference', 'levenshtein', or 'sequence_matcher'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_distance(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    return_all_metrics: bool = False\n",
    ") -> float | Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate semantic distance between two texts using sentence embeddings.\n",
    "    \n",
    "    This function uses state-of-the-art sentence transformers to create dense\n",
    "    vector representations of texts and compute their semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text string\n",
    "        text2: Second text string\n",
    "        model_name: Sentence transformer model to use. Options:\n",
    "            - 'all-MiniLM-L6-v2' (default, fast, 384 dimensions)\n",
    "            - 'all-mpnet-base-v2' (slower, more accurate, 768 dimensions)\n",
    "            - 'paraphrase-multilingual-MiniLM-L12-v2' (multilingual)\n",
    "        return_all_metrics: If True, return dict with all distance metrics\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine distance (0.0 = identical, 2.0 = opposite) if return_all_metrics=False\n",
    "        dict: Dictionary with multiple metrics if return_all_metrics=True:\n",
    "            - cosine_distance: 1 - cosine_similarity (0 to 2)\n",
    "            - cosine_similarity: Cosine similarity (-1 to 1)\n",
    "            - euclidean_distance: L2 distance between embeddings\n",
    "            - manhattan_distance: L1 distance between embeddings\n",
    "    \n",
    "    Examples:\n",
    "        >>> calculate_embedding_distance(\"The cat sits\", \"The cat sits\")\n",
    "        0.0\n",
    "        >>> calculate_embedding_distance(\"I love pizza\", \"Pizza is great\")\n",
    "        0.234\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return 1.0 if not return_all_metrics else {\n",
    "            'cosine_distance': 1.0,\n",
    "            'cosine_similarity': 0.0,\n",
    "            'euclidean_distance': 0.0,\n",
    "            'manhattan_distance': 0.0\n",
    "        }\n",
    "    \n",
    "    # Get cached model\n",
    "    calculator = EmbeddingCalculator()\n",
    "    model = calculator.get_model(model_name)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedding1 = model.encode(text1, convert_to_numpy=True)\n",
    "    embedding2 = model.encode(text2, convert_to_numpy=True)\n",
    "    \n",
    "    # Calculate cosine distance and similarity\n",
    "    cos_dist = cosine(embedding1, embedding2)\n",
    "    cos_sim = 1.0 - cos_dist\n",
    "    \n",
    "    if not return_all_metrics:\n",
    "        return float(cos_dist)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    eucl_dist = euclidean(embedding1, embedding2)\n",
    "    manh_dist = np.sum(np.abs(embedding1 - embedding2))\n",
    "    \n",
    "    return {\n",
    "        'cosine_distance': float(cos_dist),\n",
    "        'cosine_similarity': float(cos_sim),\n",
    "        'euclidean_distance': float(eucl_dist),\n",
    "        'manhattan_distance': float(manh_dist)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_translation_quality_metrics(\n",
    "    original: str,\n",
    "    translated: str,\n",
    "    model_name: str = 'all-MiniLM-L6-v2'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive quality metrics for a translation pair.\n",
    "    \n",
    "    This is a convenience function that computes both spelling error ratio\n",
    "    and embedding distance in a single call.\n",
    "    \n",
    "    Args:\n",
    "        original: Original text\n",
    "        translated: Translated text (after full translation chain)\n",
    "        model_name: Sentence transformer model to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all quality metrics:\n",
    "            - spelling_error_ratio: Word-level difference ratio\n",
    "            - embedding_distance: Semantic distance (cosine)\n",
    "            - embedding_similarity: Semantic similarity (1 - distance)\n",
    "    \n",
    "    Examples:\n",
    "        >>> metrics = calculate_translation_quality_metrics(\n",
    "        ...     \"The cat sits on the mat\",\n",
    "        ...     \"A cat is sitting on a mat\"\n",
    "        ... )\n",
    "        >>> print(f\"Error ratio: {metrics['spelling_error_ratio']:.2f}\")\n",
    "        >>> print(f\"Semantic similarity: {metrics['embedding_similarity']:.2f}\")\n",
    "    \"\"\"\n",
    "    error_ratio = calculate_spelling_error_ratio(original, translated)\n",
    "    embedding_dist = calculate_embedding_distance(original, translated, model_name)\n",
    "    \n",
    "    return {\n",
    "        'spelling_error_ratio': float(error_ratio),\n",
    "        'embedding_distance': float(embedding_dist),\n",
    "        'embedding_similarity': float(1.0 - embedding_dist)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### Spelling Error Ratio Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Identical sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"The cat sits on the mat\"\n",
    "ratio = calculate_spelling_error_ratio(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Similar sentences with different words\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"A feline rests on the rug\"\n",
    "ratio = calculate_spelling_error_ratio(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different comparison methods\n",
    "text1 = \"The weather is beautiful today\"\n",
    "text2 = \"The weather was beautiful yesterday\"\n",
    "\n",
    "for method in ['symmetric_difference', 'levenshtein', 'sequence_matcher']:\n",
    "    ratio = calculate_spelling_error_ratio(text1, text2, method=method)\n",
    "    print(f\"Method: {method:25s} -> Error Ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Distance Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Identical sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"The cat sits on the mat\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Semantically similar sentences\n",
    "text1 = \"The cat sits on the mat\"\n",
    "text2 = \"A feline rests on the rug\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different sentences\n",
    "text1 = \"I love programming in Python\"\n",
    "text2 = \"The weather is beautiful today\"\n",
    "distance = calculate_embedding_distance(text1, text2)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Embedding Distance: {distance:.4f}\")\n",
    "print(f\"Similarity: {(1-distance)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: All metrics\n",
    "text1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "text2 = \"The fast brown fox leaps over the lazy dog\"\n",
    "metrics = calculate_embedding_distance(text1, text2, return_all_metrics=True)\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(\"\\nAll Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key:25s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Quality Metrics Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comprehensive translation quality assessment\n",
    "original = \"The cat sits on the mat\"\n",
    "back_translated = \"A cat is sitting on a mat\"\n",
    "\n",
    "metrics = calculate_translation_quality_metrics(original, back_translated)\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"Back-translated: {back_translated}\")\n",
    "print(\"\\nQuality Metrics:\")\n",
    "print(f\"  Spelling Error Ratio: {metrics['spelling_error_ratio']:.4f} ({metrics['spelling_error_ratio']*100:.2f}%)\")\n",
    "print(f\"  Embedding Distance: {metrics['embedding_distance']:.4f}\")\n",
    "print(f\"  Embedding Similarity: {metrics['embedding_similarity']:.4f} ({metrics['embedding_similarity']*100:.2f}%)\")\n",
    "print(f\"\\nInterpretation: A similarity of {metrics['embedding_similarity']*100:.1f}% suggests the translation preserved meaning {'well' if metrics['embedding_similarity'] > 0.7 else 'moderately' if metrics['embedding_similarity'] > 0.5 else 'poorly'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare multiple translations\n",
    "original = \"I love programming in Python\"\n",
    "translations = [\n",
    "    \"Python programming is something I enjoy\",\n",
    "    \"I enjoy coding with Python\",\n",
    "    \"Programming in Python is fun for me\",\n",
    "    \"The weather is nice today\"  # Poor translation\n",
    "]\n",
    "\n",
    "print(f\"Original: {original}\\n\")\n",
    "for i, trans in enumerate(translations, 1):\n",
    "    metrics = calculate_translation_quality_metrics(original, trans)\n",
    "    print(f\"Translation {i}: {trans}\")\n",
    "    print(f\"  Similarity: {metrics['embedding_similarity']*100:.1f}%, Error Ratio: {metrics['spelling_error_ratio']*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"The Trump administration is eager to use the momentum of talks with Ukrainian and Russian officials to try and force both Presidents Volodymyr Zelensky and Vladimir Putin to the table on an initial ceasefire deal, those sources said\"\n",
    "errored_sentence = \"The Trump adminstration is egar to use the momemtum of talks with Ukrianian and Russain officals to try and force both President Volodymyr Zelensky and Vladmir Putin to the tabel on an inital ceasefire deal, thoze sources said.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_spelling_error_ratio(original_sentence, errored_sentence)\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence = \"The Trump administration is eager to exploit the momentum of the talks with the Ukrainian and Russian officials in order to try to force both President Volodymyr Zelensky and Vladimir Putin to the negotiating table on an initial ceasefire agreement, those sources said.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_embedding_distance(original_sentence, translated_sentence)\n",
    "print(f\" Embedding distance: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"I've successfully used the dangerous flag for extended tasks, including one memorable 9-hour session where Claude built an entire financial data analysis system. The requirements were:\"\n",
    "errored_sentence = \"I've sucessfuly uzed the dangrous flag for extanded tascs, incloding one memorabel 9 hour sesion where Claud built an entir financhal data analisis systam. The reqwirements wer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_spelling_error_ratio(original_sentence, errored_sentence)\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence = \"I successfully used the dangerous flag for extended tasks, including an unforgettable 9-hour session in which Claude built a complete financial data analysis system. The requirements were:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_embedding_distance(original_sentence, translated_sentence)\n",
    "print(f\"Embedding distance: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: long one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"The rest of the chapter deftly places us at the heart of the story by showcasing Austen’s brilliant characterisation.  We are introduced to Mr and Mrs Bennet and we understand their relationship and the inequality of their marriage.  He has become a long-suffering husband who is reduced to scoring victories through quiet sarcasm, which she is a nag, a gossip and a social climber.  Nevertheless, Austen loves them and we can see that in her affectionate treatment of their foibles.  Impressively Austen also manages to introduce through the Bennets’ conversation no less than ten other characters!  Of course we hear about the impressive Bingley and his four or five thousand a year, but we also learn that the Bennets have five grown up daughters, we hear the names and main character traits of three of them, we know which ones each parent prefers and we even have the first mention of Sir William and Lady Lucas as a rival family the Bennets need to keep up with.  There’s also a couple of side characters thrown into to the gossipy mix.  All this is done in so natural and deft a way that we happily throw ourselves into chapter two with a good basic knowledge of and fondness for the family.\"\n",
    "mispelled_sentence = \"The rest of the chaptr defly plasces us at the hart of the storry by showcasng Austen’s brilant caracterisashun. We are introdused to Mr and Mrs Bennet and we undrstand their relashunship and the inequlity of their marrige. He has becum a long sufering husbend who is reducd to scorring victoris through quiet sarkasm, while she is a nag, a gosip and a socal climber. Nevrtheless, Austen luvs them and we can see that in her afectionate treetment of their foibls. Immpresively Austen also maneges to introdce through the Bennets’ conversashun no less than ten othr charactrs. Of cours we hear about the impresive Bingley and his four or five thosand a year, but we also lern that the Bennets have five groan up daughtrs, we heer the names and main caracter trates of three of them, we know which ones each parent preffers and we even have the first menshun of Sir William and Lady Lucas as a rival fammily the Bennets need to keep up with. Ther’s also a cupple of side charactrs thrown into the gossipy mix. All this is done in so naturul and defte a way that we hapily throw ourselvs into chaptr two with a good basic knoledge of and fondness for the famly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_spelling_error_ratio(original_sentence, mispelled_sentence)\n",
    "print(f\"Error Ratio: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence = \"The rest of the chapter skillfully places us at the heart of the story while highlighting Austen's brilliant characterization. We meet Mr. and Mrs. Bennet and understand the relationship between them and the inequality in their marriage. He has become a long-suffering husband, reduced to victories through quiet sarcasm, while she is nagging, gossipy, and a social climber. Nevertheless, Austen loves them and we can see this in her affectionate treatment of their weaknesses. Impressively, Austen also manages to introduce through the Bennets' conversation no less than ten additional characters. Of course we hear about the impressive Bingley and his four or five thousand pounds a year, but we also learn that the Bennets have five grown daughters, we hear the names and main character traits of three of them, we know which ones each parent prefers, and we even have the first mention of Sir William and Lady Lucas as a rival family the Bennets need to compete with. There are also some secondary characters thrown into the gossipy mix. All this is done in such a natural and skillful way that we happily throw ourselves into the second chapter with good basic knowledge and affection for the family.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = calculate_embedding_distance(original_sentence, translated_sentence)\n",
    "print(f\"Embedding distance: {ratio:.4f} ({ratio*100:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Spelling error percentage vs embedding distance from CSV\n",
    "import os\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pandas', 'matplotlib', 'seaborn', 'numpy'])\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "csv_path = 'translation_experiments.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found: {csv_path}. Make sure it's in the repo root.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print('Columns found in CSV:', list(df.columns))\n",
    "\n",
    "# Heuristically pick columns for spelling and embedding\n",
    "cols = list(df.columns)\n",
    "spell_cols = [c for c in cols if 'spell' in c.lower() or 'error' in c.lower() or 'mis' in c.lower()]\n",
    "embed_cols = [c for c in cols if 'embed' in c.lower() or 'distance' in c.lower() or 'similar' in c.lower()]\n",
    "\n",
    "if not spell_cols:\n",
    "    raise ValueError(\"No candidate column for spelling error found in CSV. \"\n",
    "                     \"Columns must include 'spell', 'error', or 'mis' keywords.\")\n",
    "\n",
    "if not embed_cols:\n",
    "    raise ValueError(\"No candidate column for embedding distance/similarity found in CSV. \"\n",
    "                     \"Columns must include 'embed', 'distance', or 'similar' keywords.\")\n",
    "\n",
    "spell_col = spell_cols[0]\n",
    "embed_col = embed_cols[0]\n",
    "print(f'Using columns: spell=\"{spell_col}\", embed=\"{embed_col}\"')\n",
    "\n",
    "# Normalize spelling to percentage (0–100)\n",
    "spell_vals = df[spell_col].astype(float)\n",
    "if spell_vals.max() <= 1.0:\n",
    "    spelling_pct = spell_vals * 100.0\n",
    "elif spell_vals.max() <= 100.0:\n",
    "    spelling_pct = spell_vals.copy()\n",
    "else:\n",
    "    spelling_pct = 100.0 * (spell_vals - spell_vals.min()) / (spell_vals.max() - spell_vals.min())\n",
    "\n",
    "# Normalize embedding to distance\n",
    "embed_vals = df[embed_col].astype(float)\n",
    "embed_is_similarity = (\n",
    "    'similar' in embed_col.lower() or\n",
    "    (embed_vals.max() <= 1.0 and embed_vals.min() >= -1.0)\n",
    ")\n",
    "\n",
    "if embed_is_similarity:\n",
    "    embedding_distance = 1.0 - embed_vals\n",
    "else:\n",
    "    embedding_distance = embed_vals.copy()\n",
    "\n",
    "# Build dataframe\n",
    "plot_df = pd.DataFrame({\n",
    "    'spelling_pct': spelling_pct,\n",
    "    'embedding_distance': embedding_distance\n",
    "}).dropna()\n",
    "\n",
    "# Pearson correlation\n",
    "if len(plot_df) >= 2:\n",
    "    r = np.corrcoef(plot_df['spelling_pct'], plot_df['embedding_distance'])[0, 1]\n",
    "else:\n",
    "    r = np.nan\n",
    "\n",
    "# Plot\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(\n",
    "    x='spelling_pct',\n",
    "    y='embedding_distance',\n",
    "    data=plot_df,\n",
    "    scatter_kws={'s': 40, 'alpha': 0.7},\n",
    "    line_kws={'color': 'red'}\n",
    ")\n",
    "\n",
    "plt.xlabel('Spelling error (%)')\n",
    "plt.ylabel('Embedding distance (lower = more similar)')\n",
    "plt.title(f'Spelling error % vs embedding distance (r={r:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_img = 'spelling_vs_embedding.png'\n",
    "plt.savefig(out_img, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'Plot saved to: {out_img}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
